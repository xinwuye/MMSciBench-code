# MMSciBench

![License](https://img.shields.io/github/license/xinwuye/MMSciBench-code)
![Stars](https://img.shields.io/github/stars/xinwuye/MMSciBench-code?style=social)
![Issues](https://img.shields.io/github/issues/xinwuye/MMSciBench-code)

## 📌 Overview

**MMSciBench** focuses on mathematics and physics that evaluates scientific reasoning capabilities. This repository contains the code for the benchmark. The dataset is available on Hugging Face: [MMSciBench Dataset](https://huggingface.co/datasets/XinwuYe/MMSciBench).

## 📖 Paper
If you use this benchmark in your research, please cite our paper:

```
@article{}
```

## 🛠 Installation

Clone the repository:

```bash
git clone https://github.com/xinwuye/MMSciBench-code.git
cd MMSciBench-code
```

## 📊 Benchmark Dataset
The dataset for MMSciBench is available on Hugging Face:

🔗 [MMSciBench Dataset](https://huggingface.co/datasets/XinwuYe/MMSciBench)

## 🚀 Usage

### Running Evaluation
To evaluate models on the benchmark, use the following command:

```bash
python exp.py
python exp_hf.py
python eval1.py
```

## 📈 Results
Once evaluation is complete, results will be saved.

## 📜 License
This project is licensed under the [Apache-2.0 License](LICENSE).

## 🤝 Contributing
We welcome contributions! To contribute:
1. Fork the repository.
2. Create a new branch: `git checkout -b feature-branch-name`.
3. Commit your changes: `git commit -m 'Add a new feature'`.
4. Push to the branch: `git push origin feature-branch-name`.
5. Open a pull request.

## 🔗 Acknowledgments
We thank the open-source community and previous works that inspired this benchmark.

## 📬 Contact
For questions or collaborations, please open an issue.
